{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='orange'> **강화학습 • Reinforcement Learning | FINAL EXAM | 2020년12월14일 (월)**\n",
    "**<font color='red'> Melia Putri Handayani • 2019 55718 • 산업및데이터공학과 부경대학교**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='grey'> **Mountain Car Problem**\n",
    "Solve mountain car problem with linear function approximation\n",
    "Objective: \n",
    "* Implement a linear function approximation-based reinforcement learning algorithm\n",
    "* Not allow to use the non-linear function approximation and the lookup table\n",
    "* Learn a policy which can complete any episode within 300 steps\n",
    "\n",
    "Submit a source code to the ‘assignment’ board in lms\n",
    "File format: ‘JunPyoHong_2015111438.py’\n",
    "Due: 24:00, Dec. 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the Libraries\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Class Index-Hash-Table (IHT)\n",
    "class IHT:\n",
    "    \"Structure to handle collisions\"\n",
    "    def __init__(self,size_val):\n",
    "        self.size=size_val\n",
    "        self.overfull_count=0\n",
    "        self.dictionary={}\n",
    "\n",
    "    def count(self):\n",
    "        return len(self.dictionary)\n",
    "\n",
    "    def full(self):\n",
    "        return len(self.dictionary)>=self.size\n",
    "\n",
    "    def get_index(self,obj,read_only=False):\n",
    "        d=self.dictionary\n",
    "        if obj in d:\n",
    "            return d[obj]\n",
    "        elif read_only:\n",
    "            return None\n",
    "        size=self.size\n",
    "        count=self.count()\n",
    "        if count>=size:\n",
    "            if self.overfull_count == 0: print('IHT full, starting to allow collisions')\n",
    "            self.overfull_count += 1\n",
    "            return hash(obj) % self.size\n",
    "        else:\n",
    "            d[obj]=count\n",
    "            return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 'hash_coords' function\n",
    "def hash_coords(coordinates, m, read_only=False):\n",
    "    if isinstance(m, IHT): return m.get_index(tuple(coordinates),read_only)\n",
    "    if isinstance(m, int): return hash(tuple(coordinates)) % m\n",
    "    if m is None: return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 'tiles' function\n",
    "def tiles(iht_or_size,num_tilings,floats,ints=None,read_only=False):\n",
    "    \"\"\"Returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
    "    if ints is None:\n",
    "        ints=[]\n",
    "    qfloats=[floor(f*num_tilings) for f in floats]\n",
    "    tiles=[]\n",
    "    for tiling in range(num_tilings):\n",
    "        tilingX2=tiling*2\n",
    "        coords=[tiling]\n",
    "        b=tiling\n",
    "        for q in qfloats:\n",
    "            coords.append((q+b)//num_tilings)\n",
    "            b+=tilingX2\n",
    "        coords.extend(ints)\n",
    "        tiles.append(hash_coords(coords, iht_or_size, read_only))\n",
    "    return tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this point, tiles coding is ended**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Problem Description (Refer to the page 5 of the slides “6_Function Approximation_2.pdf”)**\n",
    "* A car is started at the bottom of valley.\n",
    "* For any given state, the agent may choose to accelerate to the left, right or cease any acceleration.\n",
    "\n",
    "<h5><center> **OBSERVATION** </center></h5>\n",
    "\n",
    "Index |       Observation     |  Min  |  Max \n",
    ":---: | :-------------------: | :---: | :---:\n",
    "  0   | Car position (x-axis) | -1.2  |  0.6\n",
    "  1   | Car velocity          | -0.07 |  0.07\n",
    "\n",
    "\n",
    "<h5><center> **ACTION** </center></h5>\n",
    "\n",
    "Index |          Action          \n",
    ":---: | :----------------------:\n",
    "  0   | Accelerate to the left   \n",
    "  1   | Do not accelerate        \n",
    "  2   | Accelerate to the right  \n",
    "  \n",
    "  \n",
    "**EPISODE TERMINATION**\n",
    "> The car position is more than 0.5 or episode length is greater than 1000 steps\n",
    "\n",
    "* Refer to “MountainCar_env.py” for the detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the position and velocity of observation\n",
    "position_min=-1.2\n",
    "position_max=0.5\n",
    "velocity_min=-0.07\n",
    "velocity_max=0.07\n",
    "\n",
    "#Define all possible actions (This did not work)\n",
    "#acc_left=0\n",
    "#acc_none=1\n",
    "#acc_right=2\n",
    "\n",
    "#Define all possible actions\n",
    "acc_left=-1\n",
    "acc_none=0\n",
    "acc_right=1\n",
    "\n",
    "#Define the orders of actions\n",
    "actions_order=[acc_left,acc_none,acc_right]\n",
    "\n",
    "#Use optimistic initial value, so it's ok to set epsilon to 0\n",
    "epsilon=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 'step' function\n",
    "#Take an @action at @position and @velocity\n",
    "#@return: new position, new velocity, reward (always -1)\n",
    "\n",
    "def step(position,velocity,action):\n",
    "    new_velocity=velocity+0.001*action-0.0025*np.cos(3*position)\n",
    "    new_velocity=min(max(velocity_min,new_velocity),velocity_max)\n",
    "    new_position=position+new_velocity\n",
    "    new_position=min(max(position_min, new_position),position_max)\n",
    "    reward=-1.0\n",
    "    if new_position == position_min:\n",
    "        new_velocity=0.0\n",
    "    return new_position,new_velocity,reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create wrapper class for state action value function\n",
    "class ValueFunction:\n",
    "    def __init__(self,step_size,num_of_tilings=8,max_size=2048):\n",
    "        self.max_size=max_size\n",
    "        self.num_of_tilings=num_of_tilings\n",
    "\n",
    "        #Divide step size equally to each tiling\n",
    "        self.step_size=step_size/num_of_tilings\n",
    "\n",
    "        self.hash_table=IHT(max_size)\n",
    "\n",
    "        #Weight for each tile\n",
    "        self.weights=np.zeros(max_size)\n",
    "\n",
    "        #Position and velocity needs scaling to satisfy the tile software\n",
    "        self.position_scale=self.num_of_tilings/(position_max-position_min)\n",
    "        self.velocity_scale=self.num_of_tilings/(velocity_max-velocity_min)\n",
    "\n",
    "    #Get indices of active tiles for given state and action\n",
    "    def get_active_tiles(self,position,velocity,action):\n",
    "        active_tiles=tiles(self.hash_table,self.num_of_tilings,\n",
    "                           [self.position_scale*position,self.velocity_scale*velocity],[action])\n",
    "        return active_tiles\n",
    "\n",
    "    #Estimate the value of given state and action\n",
    "    def value(self,position,velocity,action):\n",
    "        if position == position_max:\n",
    "            return 0.0\n",
    "        active_tiles=self.get_active_tiles(position,velocity,action)\n",
    "        return np.sum(self.weights[active_tiles])\n",
    "\n",
    "    #Learn with given state, action and target\n",
    "    def learn(self,position,velocity,action,target):\n",
    "        active_tiles=self.get_active_tiles(position,velocity,action)\n",
    "        estimation=np.sum(self.weights[active_tiles])\n",
    "        delta=self.step_size*(target-estimation)\n",
    "        for active_tile in active_tiles:\n",
    "            self.weights[active_tile]+=delta\n",
    "\n",
    "    #Get # of steps to reach the goal under current state value function\n",
    "    def cost_to_go(self,position,velocity):\n",
    "        costs=[]\n",
    "        for action in actions_order:\n",
    "            costs.append(self.value(position,velocity,action))\n",
    "        return -np.max(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get action at @position and @velocity based on epsilon greedy policy and @valueFunction\n",
    "def get_action(position,velocity,value_function):\n",
    "    if np.random.binomial(1,epsilon)==1:\n",
    "        return np.random.choice(actions_order)\n",
    "    values=[]\n",
    "    for action in actions_order:\n",
    "        values.append(value_function.value(position,velocity,action))\n",
    "    return np.random.choice([action_ for action_,value_ in enumerate(values) if value_==np.max(values)])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define 'semi_gradient_n_step_sarsa' function\n",
    "#Semi-gradient n-step Sarsa\n",
    "#@valueFunction: state value function to learn\n",
    "#@n: # of steps\n",
    "def semi_gradient_n_step_sarsa(value_function,n=1):\n",
    "    #Start at a random position around the bottom of the valley\n",
    "    current_position = np.random.uniform(-0.6,-0.4)\n",
    "    #Initial velocity is 0\n",
    "    current_velocity=0.0\n",
    "    #Get initial action\n",
    "    current_action=get_action(current_position,current_velocity,value_function)\n",
    "\n",
    "    #Track previous position, velocity, action and reward\n",
    "    positions=[current_position]\n",
    "    velocities=[current_velocity]\n",
    "    actions=[current_action]\n",
    "    rewards=[0.999]\n",
    "\n",
    "    #Track the time\n",
    "    time=0\n",
    "\n",
    "    #The length of this episode\n",
    "    T=float('inf')\n",
    "    while True:\n",
    "        #Go to next time step\n",
    "        time+=1\n",
    "\n",
    "        if time < T:\n",
    "            #Take current action and go to the new state\n",
    "            new_postion,new_velocity,reward=step(current_position,current_velocity,current_action)\n",
    "            #Choose new action\n",
    "            new_action=get_action(new_postion,new_velocity,value_function)\n",
    "\n",
    "            #Track new state and action\n",
    "            positions.append(new_postion)\n",
    "            velocities.append(new_velocity)\n",
    "            actions.append(new_action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if new_postion==position_max:\n",
    "                T=time\n",
    "\n",
    "        #Get the time of the state to update\n",
    "        update_time=time - n\n",
    "        if update_time >= 0:\n",
    "            returns=0.0\n",
    "            #Calculate corresponding rewards\n",
    "            for t in range(update_time+1, min(T,update_time+n)+1):\n",
    "                returns+=rewards[t]\n",
    "            #Add estimated state action value to the return\n",
    "            if update_time+n<=T:\n",
    "                returns+=value_function.value(positions[update_time+n],\n",
    "                                              velocities[update_time+n],\n",
    "                                              actions[update_time+n])\n",
    "            #Update the state value function\n",
    "            if positions[update_time] != position_max:\n",
    "                value_function.learn(positions[update_time],\n",
    "                                     velocities[update_time],\n",
    "                                     actions[update_time],\n",
    "                                     returns)\n",
    "        if update_time==T-1:\n",
    "            break\n",
    "        current_position=new_postion\n",
    "        current_velocity=new_velocity\n",
    "        current_action=new_action\n",
    "\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the funtion to print learned cost to go\n",
    "def print_cost(value_function,episode,ax):\n",
    "    grid_size=40\n",
    "    positions=np.linspace(position_min,position_max,grid_size)\n",
    "    velocities=np.linspace(velocity_min,velocity_max,grid_size)\n",
    "    axis_x=[]\n",
    "    axis_y=[]\n",
    "    axis_z=[]\n",
    "    for position in positions:\n",
    "        for velocity in velocities:\n",
    "            axis_x.append(position)\n",
    "            axis_y.append(velocity)\n",
    "            axis_z.append(value_function.cost_to_go(position,velocity))\n",
    "\n",
    "    ax.scatter(axis_x,axis_y,axis_z)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Velocity')\n",
    "    ax.set_zlabel('Cost to go')\n",
    "    ax.set_title('Episode %d' % (episode + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:24<00:00, 40.98it/s]\n"
     ]
    }
   ],
   "source": [
    "#Plotting the Results\n",
    "def figure_10_1():\n",
    "    episodes=1000\n",
    "    plot_episodes=[0,99,episodes-1]\n",
    "    fig=plt.figure(figsize=(40,10),facecolor='w')\n",
    "    axes=[fig.add_subplot(1,len(plot_episodes),i+1,projection='3d') for i in range(len(plot_episodes))]\n",
    "    num_of_tilings=8\n",
    "    alpha=0.5\n",
    "    value_function=ValueFunction(alpha,num_of_tilings)\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        semi_gradient_n_step_sarsa(value_function)\n",
    "        if ep in plot_episodes:\n",
    "            print_cost(value_function,ep,axes[plot_episodes.index(ep)])\n",
    "\n",
    "    plt.savefig('MeliaPutriHandayani_201955718.png')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    figure_10_1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
